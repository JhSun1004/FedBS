## pFedBayes
每个客户端上用BNN训练Gaussian参数（权重、偏移），然后由服务器将参数进行聚合
## FedDistill
使用知识蒸馏的方法，在每个客户端上用共同蒸馏的方式训练本地（学生）模型与教师模型，然后基于SVD的梯度近似将梯度进行压缩
## FedKD
使用交换logits（模型输出）的方式；在本地训练中将本地向量与全局平均作比较，使用交叉熵损失来计算蒸馏损失
## APFL
客户端有三个模型：本地版本的全局模型、本地模型和混合个性化模型；本地训练时，客户端同时更新本地模型和本地版本的全局模型，上传本地版全局模型；根据新的全局模型和本地模型的混合程度（由参数α控制）生成个性化模型。
## APPLE
客户端维护一个核心模型（本地模型的一部分）和本地权重向量（Directed Relationship，DR向量），只上传核心模型到服务器。个性化模型是将从服务器下载的核心模型进行加权组合，权重由本地DR向量确定。
## FedFomo
每个客户端独立优化其模型，不依赖于全局模型平均，而是基于本地目标分布评估其他客户端模型的性能，并据此加权组合模型参数。这个加权和的计算方法和更新参数的方法与我们目前用于计算和更新bias的算法有相似之处（？）（FedBSv4），但不是除以Sigma，而是除以本地模型参数与服务器上的模型参数的差。