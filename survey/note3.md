## pFedBayes
每个客户端上用BNN训练Gaussian参数（权重、偏移），然后由服务器将参数进行聚合
## FedDistill
使用知识蒸馏的方法，在每个客户端上用共同蒸馏的方式训练本地（学生）模型与教师模型，然后基于SVD的梯度近似将梯度进行压缩
## FedKD
使用交换logits（模型输出）的方式；在本地训练中将本地向量与全局平均作比较，使用交叉熵损失来计算蒸馏损失
## APFL
客户端有三个模型：本地版本的全局模型、本地模型和混合个性化模型；本地训练时，客户端同时更新本地模型和本地版本的全局模型，上传本地版全局模型；根据新的全局模型和本地模型的混合程度（由参数α控制）生成个性化模型。